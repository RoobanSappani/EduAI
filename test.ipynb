{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "with open(\"ML Notes.pdf\", \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "# page = data.pages[page_num]\n",
    "# page_image = Image.open(io.BytesIO(page.images[0].data)) # Assumes one image per page, use other logic if needed\n",
    "# extracted_text += pytesseract.image_to_string(page_image, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import io\n",
    "pdf_document = fitz.open(pdf_file)\n",
    "\n",
    "images = []\n",
    "# Loop through each page of the PDF\n",
    "for page_num in range(pdf_document.page_count):\n",
    "    page = pdf_document.load_page(page_num)\n",
    "    # Convert the page to an image\n",
    "    pix = page.get_pixmap()\n",
    "    img = Image.open(io.BytesIO(pix.tobytes()))  # Convert bytes to image\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def getArea(box):\n",
    "    return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "\n",
    "img = np.array(images[0])\n",
    "new_height,new_width = int(img.shape[0])*3, int(img.shape[1])*3\n",
    "img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n",
    "height,width = int(img.shape[0]), int(img.shape[1])\n",
    "\n",
    "# img = img[:int(h/4), :w, :]\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "ret, thresh2 = cv2.threshold(img_gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (150,2))\n",
    "mask = cv2.morphologyEx(thresh2, cv2.MORPH_DILATE, kernel)\n",
    "\n",
    "bboxes = []\n",
    "bboxes_img = img.copy()\n",
    "contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "cropped_images = []\n",
    "areas = []\n",
    "\n",
    "for cntr in contours:\n",
    "    x,y,w,h = cv2.boundingRect(cntr)\n",
    "    cv2.rectangle(bboxes_img, (x, y), (x+w, y+h), (0,0,255), 1)\n",
    "    if(h > 10):\n",
    "        cropped_images.append(img[max(0, y-15):min(y+h+15,height) , x: x+w, :])\n",
    "        areas.append(h*w)\n",
    "        bboxes.append((x,y,w,h))\n",
    "    \n",
    "print(len(cropped_images), len(areas))\n",
    "\n",
    "unsharp_images = []\n",
    "deblurred_unsharp_images = []\n",
    "for idx, cropped_image in enumerate(cropped_images):\n",
    "\n",
    "    # Apply Gaussian Blur\n",
    "    blurred = cv2.GaussianBlur(cropped_image, (9, 9), 10)\n",
    "\n",
    "    # Create an unsharp mask\n",
    "    unsharp_image = cv2.addWeighted(cropped_image, 1.5, blurred, -0.5, 0)\n",
    "    unsharp_images.append(unsharp_image)\n",
    "\n",
    "    # plt.imshow(unsharp_image)\n",
    "    # plt.show()\n",
    "\n",
    "    kernel = np.array([[0, -1, 0], \n",
    "                   [-1, 5, -1], \n",
    "                   [0, -1, 0]])\n",
    "\n",
    "    # Apply the kernel\n",
    "    deblurred_image = cv2.filter2D(cropped_image, -1, kernel)\n",
    "    deblurred_unsharp_image = cv2.filter2D(unsharp_image, -1, kernel)\n",
    "    deblurred_unsharp_images.append(deblurred_unsharp_image)\n",
    "    # cv2.imwrite(f\"unsharp_{idx}.png\", unsharp_image)\n",
    "    # cv2.imwrite(f\"deblurred_{idx}.png\", deblurred_image)\n",
    "    # cv2.imwrite(f\"deblurred_unsharp_{idx}.png\", deblurred_unsharp_image)\n",
    "    # cv2.imwrite(f\"original_{idx}.png\", cropped_image)\n",
    "\n",
    "    # plt.imshow(deblurred_image)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from matplotlib import cm\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = []\n",
    "\n",
    "for cropped_image in unsharp_images:\n",
    "\n",
    "    image = Image.fromarray(cropped_image, 'RGB')\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    text.append(generated_text)\n",
    "\n",
    "\n",
    "text = text[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# 2nd.laoso',\n",
       " 'Machine learning .',\n",
       " '# To make a machine ! learn from past experience ! and',\n",
       " 'impose , the performance of intelligent programs ...',\n",
       " '# Definition : A companion program is \" said to learn from experiment ,',\n",
       " 'with respect to task # and performance missions # it',\n",
       " 'in performance at task in its all machine by R. improves',\n",
       " 'with experience E. ( Mitchell definition ) .',\n",
       " '# AIGORMINOUTH , S.MIN. Precision Free , etc ... .',\n",
       " 'Cancer Diagnosis .',\n",
       " 'Task T. charity malignant \" benign \" rumors . \"',\n",
       " 'performance Measure \" p. \" \" \" No. of cancer patients correctly ,',\n",
       " 'obing passed ... .',\n",
       " 'Training experience , It - a database of cancer and non-oms .',\n",
       " 'spallients .',\n",
       " '2 . This Dataset :',\n",
       " '2 As a kind of effloureux . \"',\n",
       " 'by given the dataset , over , Mr. algorithm , should be able',\n",
       " 'to predict the type of the flowers ... .',\n",
       " 'displaystyle',\n",
       " 'LS Fahlins , - , Sepal , lengths , with # petal length , width ,',\n",
       " '0 0',\n",
       " '1955 Virginia Senator',\n",
       " 'It gives - Iris setosa ( Mexico California',\n",
       " '1st']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"google_gemini_api_key\"))\n",
    "llm_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "output = llm_model.generate_content(f\"\"\"I have a text read from ocr (performed on handwritten text) regarding machine learning notes taken in class. \n",
    "                                    But it is not very accurate. Give me the corrected version of the text without any explanations or assumptions.\n",
    "                                    Make sure to use your own knowledge on the subject as well when making corrections.\n",
    "                       ocr text: \"{text}\"\n",
    "                       \"\"\").text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning.\n",
      "\n",
      "To make a machine learn from past experience and improve the performance of intelligent programs.\n",
      "\n",
      "Definition: A computer program is said to learn from experience E with respect to some task T and performance measure P, if its performance at T, as measured by P, improves with experience E. (Mitchell definition).\n",
      "\n",
      "Algorithms: SVM, MIN, Precision, Recall, etc.\n",
      "\n",
      "Cancer Diagnosis:\n",
      "\n",
      "Task T: Classify malignant \"cancer\" or benign \"tumors\".\n",
      "\n",
      "Performance Measure P: Number of cancer patients correctly diagnosed.\n",
      "\n",
      "Training Experience E: A database of cancer and non-cancer patients.\n",
      "\n",
      "2.  This Dataset:\n",
      "\n",
      "As a type of classifier.\n",
      "\n",
      "Given a dataset, an algorithm should be able to predict the type of flowers.\n",
      "\n",
      "Features: Sepal length, sepal width, petal length, petal width.\n",
      "\n",
      "Iris dataset: Iris setosa (Mexico, California)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/roobansappani/Desktop/EduAI/EduAI/gc_vision_creds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected text:\n",
      "27/07/20\n",
      "Machine\n",
      "Learning\n",
      "-\n",
      "To make\n",
      "a\n",
      "machine\n",
      "learn from past experiences\n",
      "and\n",
      "Definition: A computer program\n",
      "improve the performance of intelligent programs.\n",
      "is said to learn from experienceE\n",
      "with respect\n",
      "to\n",
      "its\n",
      "task T and perfor\n",
      "performane at task in T, as\n",
      "mance\n",
      "we as une\n",
      "by\n",
      "measure f\n",
      "improves\n",
      "if\n",
      "with\n",
      "experience E. (Mitchell definition).\n",
      "->>\n",
      "Algorithms: KHN, SVM, Desicion Izee\n",
      "etc.\n",
      "Task T-\n",
      "Cancer Diagnosis:\n",
      "Classify malignant / benign\n",
      "tumors\n",
      "Performance Measure\n",
      "P\n",
      "-\n",
      "\"No.\n",
      "of\n",
      "cancer patients correctly\n",
      "diagnosed...\n",
      "Training\n",
      "patienti\n",
      "Iris Dataset:\n",
      "experience E-\n",
      "a database of\n",
      "cancer and non-conay\n",
      "↳ 3 kind\n",
      "of flowers.\n",
      "Ly\n",
      "given\n",
      "the\n",
      "dataret, our\n",
      "ML algorithm should\n",
      "be able\n",
      "to perdict the type of the flower.\n",
      "↳ Features\n",
      "-\n",
      "Sepal length, width & petal length, width\n",
      "4 types\n",
      "-\n",
      "Iris setosa 10) Versicolor (1), Virginica (2)\n",
      "Scanned with CamScanner\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the image using PIL\n",
    "image = images[0]\n",
    "\n",
    "# Convert the PIL image to a byte array\n",
    "byte_arr = io.BytesIO()\n",
    "image.save(byte_arr, format=image.format)\n",
    "byte_arr = byte_arr.getvalue()\n",
    "\n",
    "# Initialize the Vision API client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# Create a Google Cloud Vision Image object\n",
    "gcv_image = vision.Image(content=byte_arr)\n",
    "\n",
    "\n",
    "# Perform text detection\n",
    "response = client.text_detection(image=gcv_image)\n",
    "annotations = response.text_annotations\n",
    "\n",
    "# Extract and print the detected text\n",
    "if annotations:\n",
    "    print(\"Detected text:\")\n",
    "    print(annotations[0].description)\n",
    "else:\n",
    "    print(\"No text detected.\")\n",
    "\n",
    "# Check for errors\n",
    "if response.error.message:\n",
    "    raise Exception(f\"API error: {response.error.message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "with open(\"temp.pdf\", \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    text = \"\".join(page.extract_text() for page in reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose of closing the \n",
      "accounts. \n",
      "This document has a strictly defined form. \n",
      "It should be performed in a reliable and comparable manner so that its \n",
      "addressee can efficiently read all the necessary information.\n",
      "The financial statements shall consist of:\n",
      "1) balance sheet;\n",
      "(2)the profit and loss account;\n",
      "(3)notes, including an introduction to the financial statements and \n",
      "additional information and explanations.Scope of the reportFinancial statements of the entity73The report shall be distinguished by:\n",
      "Credibility (reliability),\n",
      "Intelligibility for users,\n",
      "Completeness\n",
      "Comparability ,\n",
      "Verifiability ,\n",
      "Punctuality ,\n",
      "Business continuity.\n",
      "•Investors \n",
      "•creditors using reports when making decisions on granting a loan, bank \n",
      "loan or sale with a deferred payment date,\n",
      "•government agencies and local governments, using reports when assessing \n",
      "the compliance of the company's activities with applicable legal \n",
      "regulations,\n",
      "•competition using reports when assessing strengths, weaknesses and the \n",
      "company's competitive strategy,\n",
      "•a manager using reports in decisions related to the management of the \n",
      "company.Features of the financial statements\n",
      "Recipients of financial statementsBALANCE SHEET\n",
      "The balance sheet is the basic financial statements of the enterprise, which \n",
      "present the state of assets and sources of their financing at a given moment. \n",
      "It is, in a sense, a \"photograph\" of the company, presenting its static image.\n",
      "BALANCE SHEET\n",
      "Therefore, it should meet the formal requirements, i.e. include:\n",
      "identification of the entity drawing up the balance sheet,\n",
      "indication of the balance sheet moment,\n",
      "specification of the intended items, groups and their values,\n",
      "signatures of persons responsible for the fair and correct preparation of the \n",
      "balance sheet,\n",
      "the date on which the balance sheet was drawn up. \n",
      "BALANCE SHEET\n",
      "The above -mentioned Act imposes on business entities keeping accounting \n",
      "books the obligation to prepare a balance sheet at a specific date, the so -\n",
      "called balance sheet date, which may be:\n",
      "the day which ends the financial year (31 December),\n",
      "the date on which the activity ceases in connection with the sale of the \n",
      "company, the termination of liquidation or bankruptcy proceedings,\n",
      "the day preceding the change of legal form, putting the entity in liquidation \n",
      "or bankruptcy,\n",
      "another date on which the entity is obliged to draw up a balance sheet and \n",
      "other financial statements (e.g. joint -stock companies listed on the stock \n",
      "exchange are obliged to present quarterly reports).\n",
      "BALANCE SHEET\n",
      "The layout of the balance sheet shall be governed by the relevant provisions. \n",
      "Assets and liabilities are included in a specific order and combined into groups \n",
      "with similar economic content. The construction of the balance sheet in \n",
      "Poland has changed several times. Currently, a system similar to balance \n",
      "sheet layouts in the European Union countries is used. \n",
      "The order of assets is ranked according to increasing liquidity, that is, \n",
      "according to the possibility (ease) of converting them into cash. At the \n",
      "beginning of the assets, fixed assets are given, which are less liquid than \n",
      "current assets. On the liabilities side, the individual components are ordered \n",
      "according to the degree of urgency of their return to creditors and owners \n",
      "(repayment dates). \n",
      "Profit and LossStatement\n",
      "Profit and loss (P&L) statement refers to a financial statement that \n",
      "summarizes the revenues, costs, and expenses incurred during a specified \n",
      "period, usually a quarter or fiscal year. These records provide information \n",
      "about a company’s ability or inability to generate profit by increasing \n",
      "revenue, reducing costs, or both. \n",
      "P&L statements are often presented on a cash or accrual basis. Company \n",
      "managers and investors use P&L statements to analyze the financial health of \n",
      "a company.\n",
      "Profit and LossStatement\n",
      "The P&L statement is one of three financial statements that every public company \n",
      "issues on a quarterly and annual basis, along with the balance sheet and the cash \n",
      "flow statement. It is often the most popular and common financial statement in a \n",
      "business plan, as it shows how much profit or loss was generated by a business.\n",
      "P&L statements are also referred to as a(n):\n",
      "Statement of profit and loss\n",
      "Statement of operations\n",
      "Statement of financial results or income\n",
      "Earnings statement\n",
      "Expense statement\n",
      "Income statement\n",
      "Profit and LossStatement\n",
      "The income statement shall compare the revenues from the various activities \n",
      "and the related costs. When drawing up the profit and loss account, it is \n",
      "necessary to be guided by the principle of proportionality. \n",
      "When the difference between revenues and costs is calculated, we get an \n",
      "accurate picture of the current financial result of a given business entity. This \n",
      "can be a gross profit or loss. The obtained gross result still needs to be \n",
      "modified with mandatory charges, such as income tax. Then we obtain the \n",
      "full net financial result of a given company within a specified period of time. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6359"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.count(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vision\n\u001b[0;32m----> 3\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageAnnotatorClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mImage(content\u001b[38;5;241m=\u001b[39munsharp_images[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mdocument_text_detection(image\u001b[38;5;241m=\u001b[39mimage)\n",
      "File \u001b[0;32m~/Desktop/EduAI/venv/lib/python3.11/site-packages/google/cloud/vision_v1/services/image_annotator/client.py:679\u001b[0m, in \u001b[0;36mImageAnnotatorClient.__init__\u001b[0;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[1;32m    671\u001b[0m     transport_init: Union[\n\u001b[1;32m    672\u001b[0m         Type[ImageAnnotatorTransport], Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ImageAnnotatorTransport]\n\u001b[1;32m    673\u001b[0m     ] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ImageAnnotatorTransport], transport)\n\u001b[1;32m    677\u001b[0m     )\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m \u001b[43mtransport_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_cert_source_for_mtls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_cert_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport):\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CLIENT_LOGGING_SUPPORTED \u001b[38;5;129;01mand\u001b[39;00m _LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(\n\u001b[1;32m    693\u001b[0m         std_logging\u001b[38;5;241m.\u001b[39mDEBUG\n\u001b[1;32m    694\u001b[0m     ):  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EduAI/venv/lib/python3.11/site-packages/google/cloud/vision_v1/services/image_annotator/transports/grpc.py:238\u001b[0m, in \u001b[0;36mImageAnnotatorGrpcTransport.__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_channel_credentials \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials(\n\u001b[1;32m    234\u001b[0m                 certificate_chain\u001b[38;5;241m=\u001b[39mcert, private_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[1;32m    235\u001b[0m             )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     channel_init \u001b[38;5;241m=\u001b[39m channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreate_channel\n",
      "File \u001b[0;32m~/Desktop/EduAI/venv/lib/python3.11/site-packages/google/cloud/vision_v1/services/image_annotator/transports/base.py:103\u001b[0m, in \u001b[0;36mImageAnnotatorTransport.__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mload_credentials_from_file(\n\u001b[1;32m    100\u001b[0m         credentials_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_credentials:\n\u001b[0;32m--> 103\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_gdch_audience\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/EduAI/venv/lib/python3.11/site-packages/google/auth/_default.py:697\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    689\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    693\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[1;32m    694\u001b[0m             )\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[0;32m--> 697\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "\n",
    "image = vision.Image(content=unsharp_images[0])\n",
    "\n",
    "response = client.document_text_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"google_gemini_api_key\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_test_question_template = \"\"\"\n",
    "You are an Education AI assistant tailored to make mock questions and answers for students so that they can prepare for their exam. \n",
    "You will be preparing these mock questions and answers based on the context given in the <context> tags.\n",
    "Make sure to have a mix of MCQ, written and one word qnswer questions.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "When you prepare the questions make sure to have it in the format given in the <question_format>.\n",
    "\n",
    "<question>\n",
    "1. question 1\n",
    "2. question 2\n",
    ".\n",
    ".\n",
    "</question>\n",
    "\n",
    "First 3 questions are MCQ, \n",
    "Next 3 are one word answers\n",
    "Next 4 are general.\n",
    "Prepare questions in the given format based on the given context.\n",
    "\n",
    "Qestions:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = mock_test_question_template.format(context = text)\n",
    "\n",
    "response = model.generate_content(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<question>\n",
      "1.  What are the three components of the accounting system as outlined in the text?\n",
      "\n",
      "2. Multiple Choice: The balance sheet method in accounting is based on which principle?\n",
      "    a) Single-entry bookkeeping\n",
      "    b) Double-entry bookkeeping\n",
      "    c) Triple-entry bookkeeping\n",
      "    d) None of the above\n",
      "\n",
      "3. What is the subjective method in accounting, and provide an example illustrating its application?\n",
      "\n",
      "4.  What is the basic legal act regulating the accounting system in most countries?\n",
      "\n",
      "5. What are the three criteria that must be met for an asset to be included in the balance sheet?\n",
      "\n",
      "6. Define 'equity' as it relates to accounting.\n",
      "\n",
      "7.  What is the difference between an economic event and an economic operation?  Give an example of each.\n",
      "\n",
      "8.  One-word answer: What type of accounting involves the anticipation of tax burdens and informing about current obligations to the tax office?\n",
      "\n",
      "9.  What are the key functions of accounting books, differentiating between accounting books and tax books?\n",
      "\n",
      "10. What are the essential components of a company chart of accounts?  Why is it important?\n",
      "\n",
      "</question>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
